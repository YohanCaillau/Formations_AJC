{"cells":[{"cell_type":"markdown","source":["# Structured Streaming with Azure EventHubs \n\n## Datasets Used\nThis notebook will consumn data being published through an EventHub with the following schema:\n\n- `Index`\n- `Arrival_Time`\n- `Creation_Time`\n- `x`\n- `y`\n- `z`\n- `User`\n- `Model`\n- `Device`\n- `gt`\n- `id`\n- `geolocation`\n\n## Library Requirements\n\n1. the Maven library with coordinate `com.microsoft.azure:azure-eventhubs-spark_2.12:2.3.18`\n   - this allows Databricks `spark` session to communicate with an Event Hub\n2. the Python library `azure-eventhub`\n   - this is allows the Python kernel to stream content to an Event Hub\n\nThe next cell walks you through installing the Maven library. A couple cells below that, we automatically install the Python library using `%pip install`."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ecf745db-3d5a-4918-8d07-f3a5d3ccdbf5"}}},{"cell_type":"markdown","source":["## Lab Setup\n\nIf you are running in an Azure Databricks environment that is already pre-configured with the libraries you need, you can skip to the next cell. To use this notebook in your own Databricks environment, you will need to create libraries, using the [Create Library](https://docs.azuredatabricks.net/user-guide/libraries.html) interface in Azure Databricks. Follow the steps below to attach the `azure-eventhubs-spark` library to your cluster:\n\n1. In the left-hand navigation menu of your Databricks workspace, select **Compute**, then select your cluster in the list. If it's not running, start it now.\n\n  ![Select cluster](https://databricksdemostore.blob.core.windows.net/images/10-de-learning-path/select-cluster.png)\n\n2. Select the **Libraries** tab (1), then select **Install New** (2). In the Install Library dialog, select **Maven** under Library Source (3). Under Coordinates, paste **com.microsoft.azure:azure-eventhubs-spark_2.12:2.3.18** (4), then select **Install**.\n  \n  ![Databricks new Maven library](https://databricksdemostore.blob.core.windows.net/images/10-de-learning-path/install-eventhubs-spark-library.png)\n\n3. Wait until the library successfully installs before continuing.\n\n  ![Library installed](https://databricksdemostore.blob.core.windows.net/images/10-de-learning-path/eventhubs-spark-library-installed.png)\n\nOnce complete, return to this notebook to continue with the lesson."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e3daddb4-ab6a-4229-9529-c3f0d6061009"}}},{"cell_type":"markdown","source":["### Getting Started\n\nRun the following two cells to install the `azure-eventhub` Python library and configure our \"classroom.\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"221bffa2-02b4-4cd0-84ae-721a3b895a94"}}},{"cell_type":"code","source":["# This library allows the Python kernel to stream content to an Event Hub:\n%pip install azure-eventhub"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8150325f-906b-45b3-bd3b-02e24cec852a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Python interpreter will be restarted.\nCollecting azure-eventhub\n  Downloading azure_eventhub-5.10.1-py3-none-any.whl (150 kB)\nCollecting uamqp<2.0.0,>=1.6.0\n  Downloading uamqp-1.6.0-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.0 MB)\nCollecting azure-core<2.0.0,>=1.14.0\n  Downloading azure_core-1.25.1-py3-none-any.whl (178 kB)\nCollecting typing-extensions>=4.0.1\n  Downloading typing_extensions-4.3.0-py3-none-any.whl (25 kB)\nRequirement already satisfied: six>=1.11.0 in /databricks/python3/lib/python3.9/site-packages (from azure-core<2.0.0,>=1.14.0->azure-eventhub) (1.16.0)\nRequirement already satisfied: requests>=2.18.4 in /databricks/python3/lib/python3.9/site-packages (from azure-core<2.0.0,>=1.14.0->azure-eventhub) (2.26.0)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.9/site-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.14.0->azure-eventhub) (3.2)\nRequirement already satisfied: charset-normalizer~=2.0.0 in /databricks/python3/lib/python3.9/site-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.14.0->azure-eventhub) (2.0.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /databricks/python3/lib/python3.9/site-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.14.0->azure-eventhub) (1.26.7)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.9/site-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.14.0->azure-eventhub) (2021.10.8)\nInstalling collected packages: typing-extensions, uamqp, azure-core, azure-eventhub\n  Attempting uninstall: typing-extensions\n    Found existing installation: typing-extensions 3.10.0.2\n    Not uninstalling typing-extensions at /databricks/python3/lib/python3.9/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-3d04e954-c5ce-4d1c-8208-1193abd000d4\n    Can't uninstall 'typing-extensions'. No files were found to uninstall.\nSuccessfully installed azure-core-1.25.1 azure-eventhub-5.10.1 typing-extensions-4.3.0 uamqp-1.6.0\nPython interpreter will be restarted.\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Python interpreter will be restarted.\nCollecting azure-eventhub\n  Downloading azure_eventhub-5.10.1-py3-none-any.whl (150 kB)\nCollecting uamqp<2.0.0,>=1.6.0\n  Downloading uamqp-1.6.0-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.0 MB)\nCollecting azure-core<2.0.0,>=1.14.0\n  Downloading azure_core-1.25.1-py3-none-any.whl (178 kB)\nCollecting typing-extensions>=4.0.1\n  Downloading typing_extensions-4.3.0-py3-none-any.whl (25 kB)\nRequirement already satisfied: six>=1.11.0 in /databricks/python3/lib/python3.9/site-packages (from azure-core<2.0.0,>=1.14.0->azure-eventhub) (1.16.0)\nRequirement already satisfied: requests>=2.18.4 in /databricks/python3/lib/python3.9/site-packages (from azure-core<2.0.0,>=1.14.0->azure-eventhub) (2.26.0)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.9/site-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.14.0->azure-eventhub) (3.2)\nRequirement already satisfied: charset-normalizer~=2.0.0 in /databricks/python3/lib/python3.9/site-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.14.0->azure-eventhub) (2.0.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /databricks/python3/lib/python3.9/site-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.14.0->azure-eventhub) (1.26.7)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.9/site-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.14.0->azure-eventhub) (2021.10.8)\nInstalling collected packages: typing-extensions, uamqp, azure-core, azure-eventhub\n  Attempting uninstall: typing-extensions\n    Found existing installation: typing-extensions 3.10.0.2\n    Not uninstalling typing-extensions at /databricks/python3/lib/python3.9/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-3d04e954-c5ce-4d1c-8208-1193abd000d4\n    Can't uninstall 'typing-extensions'. No files were found to uninstall.\nSuccessfully installed azure-core-1.25.1 azure-eventhub-5.10.1 typing-extensions-4.3.0 uamqp-1.6.0\nPython interpreter will be restarted.\n"]}}],"execution_count":0},{"cell_type":"code","source":["%run \"./Includes/Classroom-Setup\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bf492a64-6282-4987-be4f-cb1dfc5d4577"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Initialized classroom variables & functions...","textData":null,"removedWidgets":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"type":"htmlSandbox","arguments":{}}},"output_type":"display_data","data":{"text/html":["Initialized classroom variables & functions..."]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Datasets are already mounted to <b>/mnt/training</b> from <b>wasbs://training@dbtraineastasia.blob.core.windows.net/</b>","textData":null,"removedWidgets":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"type":"htmlSandbox","arguments":{}}},"output_type":"display_data","data":{"text/html":["Datasets are already mounted to <b>/mnt/training</b> from <b>wasbs://training@dbtraineastasia.blob.core.windows.net/</b>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Created user-specific database","textData":null,"removedWidgets":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"type":"htmlSandbox","arguments":{}}},"output_type":"display_data","data":{"text/html":["Created user-specific database"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Using the database <b style=\"color:green\">xlab_czy_953_xtremelabs_us_db</b>.","textData":null,"removedWidgets":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"type":"htmlSandbox","arguments":{}}},"output_type":"display_data","data":{"text/html":["Using the database <b style=\"color:green\">xlab_czy_953_xtremelabs_us_db</b>."]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"All done!","textData":null,"removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"htmlSandbox","arguments":{}}},"output_type":"display_data","data":{"text/html":["All done!"]}}],"execution_count":0},{"cell_type":"markdown","source":["The following cell sets up a local streaming file read that we'll be writing to Event Hubs."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"08b5a1f9-1090-4313-977d-1855881e503e"}}},{"cell_type":"code","source":["%run ./Includes/Streaming-Demo-Setup"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"aa5948ac-eb66-49e8-acff-1fbdf04a83aa"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["In order to reach Event Hubs, you will need to insert the connection string-primary key you acquired at the end of the Getting Started notebook in this module. You acquired this from the Azure Portal, and copied it into Notepad.exe or another text editor.\n\n> Read this article to learn [how to acquire the connection string for an Event Hub](https://docs.microsoft.com/azure/event-hubs/event-hubs-create) in your own Azure Subscription."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7a7abade-e10a-430c-9f1e-09c1b1c0349f"}}},{"cell_type":"code","source":["event_hub_connection_string = \"Zci9JpEcEWtKHsSZr27i/Qq3z2zi8lNgXXBVox1k7n4=\" # Paste your Event Hubs connection string in the quotes to the left"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"85c323e0-5272-4216-bc51-80e980065c31"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["<h2><img src=\"https://files.training.databricks.com/images/105/logo_spark_tiny.png\"> Azure Event Hubs</h2>\n\nMicrosoft Azure Event Hubs is a fully managed, real-time data ingestion service.\nYou can stream millions of events per second from any source to build dynamic data pipelines and immediately respond to business challenges.\nIt integrates seamlessly with a host of other Azure services.\n\nEvent Hubs can be used in a variety of applications such as\n* Anomaly detection (fraud/outliers)\n* Application logging\n* Analytics pipelines, such as clickstreams\n* Archiving data\n* Transaction processing\n* User telemetry processing\n* Device telemetry streaming\n* <b>Live dashboarding</b>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8c0760e1-9bad-49ca-aefd-10a4dbe72845"}}},{"cell_type":"code","source":["%python\n\nevent_hub_name = \"databricks-demo-eventhub\"\nconnection_string = event_hub_connection_string + \";EntityPath=\" + event_hub_name\n\nprint(\"Consumer Connection String: {}\".format(connection_string))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"093c02f4-e628-4977-9cf0-f689aa329cb4"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Consumer Connection String: Zci9JpEcEWtKHsSZr27i/Qq3z2zi8lNgXXBVox1k7n4=;EntityPath=databricks-demo-eventhub\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Consumer Connection String: Zci9JpEcEWtKHsSZr27i/Qq3z2zi8lNgXXBVox1k7n4=;EntityPath=databricks-demo-eventhub\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Write Stream to Event Hub to Produce Stream"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0e671cbb-ecaf-4d1f-83e7-6bfeb75ff234"}}},{"cell_type":"code","source":["%python\n\n# For 2.3.15 version and above, the configuration dictionary requires that connection string be encrypted.\nehWriteConf = {'eventhubs.connectionString' : sc._jvm.org.apache.spark.eventhubs.EventHubsUtils.encrypt(connection_string)}\n\ncheckpointPath = userhome + \"/event-hub/write-checkpoint\"\ndbutils.fs.rm(checkpointPath,True)\n\n(activityStreamDF\n  .writeStream\n  .format(\"eventhubs\")\n  .options(**ehWriteConf)\n  .option(\"checkpointLocation\", checkpointPath)\n  .start())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6ffe932b-c2bb-45b1-a187-990fb8145840"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)\n\u001B[0;32m<command-1918969016947192>\u001B[0m in \u001B[0;36m<cell line: 3>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;31m# For 2.3.15 version and above, the configuration dictionary requires that connection string be encrypted.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      2\u001B[0m ehWriteConf = {\n\u001B[0;32m----> 3\u001B[0;31m   \u001B[0;34m'eventhubs.connectionString'\u001B[0m \u001B[0;34m:\u001B[0m \u001B[0msc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jvm\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0morg\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mapache\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0meventhubs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mEventHubsUtils\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mencrypt\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mconnection_string\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      4\u001B[0m }\n\u001B[1;32m      5\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mTypeError\u001B[0m: 'JavaPackage' object is not callable","errorSummary":"<span class='ansi-red-fg'>TypeError</span>: 'JavaPackage' object is not callable","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)\n\u001B[0;32m<command-1918969016947192>\u001B[0m in \u001B[0;36m<cell line: 3>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;31m# For 2.3.15 version and above, the configuration dictionary requires that connection string be encrypted.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      2\u001B[0m ehWriteConf = {\n\u001B[0;32m----> 3\u001B[0;31m   \u001B[0;34m'eventhubs.connectionString'\u001B[0m \u001B[0;34m:\u001B[0m \u001B[0msc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jvm\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0morg\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mapache\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0meventhubs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mEventHubsUtils\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mencrypt\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mconnection_string\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      4\u001B[0m }\n\u001B[1;32m      5\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mTypeError\u001B[0m: 'JavaPackage' object is not callable"]}}],"execution_count":0},{"cell_type":"markdown","source":["<h2><img src=\"https://files.training.databricks.com/images/105/logo_spark_tiny.png\"> Event Hubs Configuration</h2>\n\nAssemble the following:\n* A `startingEventPosition` as a JSON string\n* An `EventHubsConf`\n  * to include a string with connection credentials\n  * to set a starting position for the stream read\n  * to throttle Event Hubs' processing of the streams"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8c02e996-b843-4673-9f6e-60e83ee8ca21"}}},{"cell_type":"code","source":["%python\n\nimport json\n\n# Create the starting position Dictionary\nstartingEventPosition = {\n  \"offset\": \"-1\",\n  \"seqNo\": -1,            # not in use\n  \"enqueuedTime\": None,   # not in use\n  \"isInclusive\": True\n}\n\neventHubsConf = {\n  \"eventhubs.connectionString\" : sc._jvm.org.apache.spark.eventhubs.EventHubsUtils.encrypt(connection_string),\n  \"eventhubs.startingPosition\" : json.dumps(startingEventPosition),\n  \"setMaxEventsPerTrigger\": 100\n}"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"11c4cda5-e187-4f83-b259-90f33f317b6e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"markdown","source":["### READ Stream using Event Hubs\n\nThe `readStream` method is a <b>transformation</b> that outputs a DataFrame with specific schema specified by `.schema()`."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"306ebc06-09ec-42bf-a0cd-83b52a8ac9e2"}}},{"cell_type":"code","source":["%python\n\nfrom pyspark.sql.functions import col\n\nspark.conf.set(\"spark.sql.shuffle.partitions\", sc.defaultParallelism)\n\neventStreamDF = (spark.readStream\n  .format(\"eventhubs\")\n  .options(**eventHubsConf)\n  .load()\n)\n\neventStreamDF.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"62095865-6099-417d-8263-1941cbbda4c9"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"markdown","source":["Most of the fields in this response are metadata describing the state of the Event Hubs stream. We are specifically interested in the `body` field, which contains our JSON payload.\n\nNoting that it's encoded as binary, as we select it, we'll cast it to a string."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"57d7a5ea-44b1-4e32-aa1c-8615ffc25d6e"}}},{"cell_type":"code","source":["%python\nbodyDF = eventStreamDF.select(col(\"body\").cast(\"STRING\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"868d0984-9d06-4897-8f52-044eca4ae49f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"markdown","source":["Each line of the streaming data becomes a row in the DataFrame once an <b>action</b> such as `writeStream` is invoked.\n\nNotice that nothing happens until you engage an action, i.e. a `display()` or `writeStream`."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"907be8a8-4ba6-4e88-b6e0-2b4377c60f09"}}},{"cell_type":"code","source":["%python\ndisplay(bodyDF, streamName= \"bodyDF\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ee8cdb1a-9d62-4ba3-9722-c89dec5e6321"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"markdown","source":["While we can see our JSON data now that it's cast to string type, we can't directly manipulate it.\n\nBefore proceeding, stop this stream. We'll continue building up transformations against this streaming DataFrame, and a new action will trigger an additional stream."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1af09ffa-a04d-4d37-b8d3-2348c781e09f"}}},{"cell_type":"code","source":["%python\nfor s in spark.streams.active:\n  if s.name == \"bodyDF\":\n    s.stop()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"44a1b348-2e8d-4aab-b3f0-d140a0fc8813"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"markdown","source":["## <img src=\"https://files.training.databricks.com/images/105/logo_spark_tiny.png\"> Parse the JSON payload\n\nThe Event Hub acts as a sort of \"firehose\" (or asynchronous buffer) and displays raw data in the JSON format.\n\nIf desired, we could save this as raw bytes or strings and parse these records further downstream in our processing.\n\nHere, we'll directly parse our data so we can interact with the fields.\n\nThe first step is to define the schema for the JSON payload.\n\n> Both time fields are encoded as `LongType` here because of non-standard formatting."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1e6a7129-35de-4a2e-9c09-b24664dd2346"}}},{"cell_type":"code","source":["%python\n\nfrom pyspark.sql.types import StructField, StructType, StringType, LongType, DoubleType\n\nschema = StructType([\n  StructField(\"Arrival_Time\", LongType(), True),\n  StructField(\"Creation_Time\", LongType(), True),\n  StructField(\"Device\", StringType(), True),\n  StructField(\"Index\", LongType(), True),\n  StructField(\"Model\", StringType(), True),\n  StructField(\"User\", StringType(), True),\n  StructField(\"gt\", StringType(), True),\n  StructField(\"x\", DoubleType(), True),\n  StructField(\"y\", DoubleType(), True),\n  StructField(\"z\", DoubleType(), True),\n  StructField(\"geolocation\", StructType([\n    StructField(\"PostalCode\", StringType(), True),\n    StructField(\"StateProvince\", StringType(), True),\n    StructField(\"city\", StringType(), True),\n    StructField(\"country\", StringType(), True)\n  ]), True),\n  StructField(\"id\", StringType(), True)\n])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d7e0476b-b6b2-4476-ac3d-7ebf78e1a2bd"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"markdown","source":["### Parse the data\n\nNext we can use the function `from_json` to parse out the full message with the schema specified above.\n\nWhen parsing a value from JSON, we end up with a single column containing a complex object."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0eac6d5e-aecc-48a2-87b0-2b86185b2c45"}}},{"cell_type":"code","source":["%python\n\nfrom pyspark.sql.functions import col, from_json\n\nparsedEventsDF = bodyDF.select(\n  from_json(col(\"body\"), schema).alias(\"json\"))\n\nparsedEventsDF.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3db04f92-9469-422f-9c61-4b0349f7eb5e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"markdown","source":["Note that we can further parse this to flatten the schema entirely and properly cast our time fields."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"02870873-f182-4316-821b-3e51306aeeec"}}},{"cell_type":"code","source":["%python\n\nfrom pyspark.sql.functions import from_unixtime\n\nflatSchemaDF = (parsedEventsDF\n  .select(from_unixtime(col(\"json.Arrival_Time\")/1000).alias(\"Arrival_Time\").cast(\"timestamp\"),\n          (col(\"json.Creation_Time\")/1E9).alias(\"Creation_Time\").cast(\"timestamp\"),\n          col(\"json.Device\").alias(\"Device\"),\n          col(\"json.Index\").alias(\"Index\"),\n          col(\"json.Model\").alias(\"Model\"),\n          col(\"json.User\").alias(\"User\"),\n          col(\"json.gt\").alias(\"gt\"),\n          col(\"json.x\").alias(\"x\"),\n          col(\"json.y\").alias(\"y\"),\n          col(\"json.z\").alias(\"z\"),\n          col(\"json.id\").alias(\"id\"),\n          col(\"json.geolocation.country\").alias(\"country\"),\n          col(\"json.geolocation.city\").alias(\"city\"),\n          col(\"json.geolocation.PostalCode\").alias(\"PostalCode\"),\n          col(\"json.geolocation.StateProvince\").alias(\"StateProvince\"))\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3e1bb285-3d6e-4d67-b2b7-98af9ce2b4b1"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"markdown","source":["This flat schema provides us the ability to view each nested field as a column."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"aeff8317-7136-4082-8cca-9c84309beb6b"}}},{"cell_type":"code","source":["%python\ndisplay(flatSchemaDF)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ba4e84e2-549d-46e8-9b92-bdf393bc9021"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"markdown","source":["### Stop all active streams"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"207c16fc-0185-4cfc-8a0f-ecde383574ca"}}},{"cell_type":"code","source":["%python\nfor s in spark.streams.active:\n  s.stop()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b4b1df07-a435-45b8-911b-d500f64bba37"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"markdown","source":["#### Event Hubs FAQ\n\nThis [FAQ](https://github.com/Azure/azure-event-hubs-spark/blob/master/FAQ.md) can be an invaluable reference for occasional Spark-EventHub debugging."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"01ba235e-0392-4425-b7e4-15c20047dd56"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"3.Streaming-With-Event-Hubs-Demo","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":1918969016947179}},"nbformat":4,"nbformat_minor":0}
