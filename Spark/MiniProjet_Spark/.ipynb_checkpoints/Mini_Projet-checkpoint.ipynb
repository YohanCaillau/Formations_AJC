{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "id1OA_Rdh2gE",
    "outputId": "e44e2900-5d09-44a7-ed7f-e57ad93354fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: findspark in /usr/local/lib/python3.7/dist-packages (2.0.1)\n"
     ]
    }
   ],
   "source": [
    "#!pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "p-CIpVyvh_MX"
   },
   "outputs": [],
   "source": [
    "#!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "R3vBmZ8cnLpH"
   },
   "outputs": [],
   "source": [
    "#!pip install kafka-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "0UtT2UyohoDU"
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import Row\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql.functions import from_json\n",
    "from pyspark.sql.functions import schema_of_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "sAV39ONNzL8a"
   },
   "outputs": [],
   "source": [
    "#Initialisation de findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "ODm6ttDOobvq"
   },
   "outputs": [],
   "source": [
    "spark = (SparkSession.builder.appName(\"Kafka Pyspark Streaming Learning\")\n",
    "        .master(\"local[*]\")\n",
    "        .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0\")\n",
    "        .getOrCreate())\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "id": "TuwC3PB0qIIO",
    "outputId": "7a1bcff8-3c65-4dab-cdc3-c491447382c6"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'\\nKAFKA_TOPIC_NAME = \"vlib_status_ville\"\\nKAFKA_BOOTSTRAP_SERVER = \"51.38.185.58:9092\"\\n    \\ndf = (\\n    spark.readStream.format(\"kafka\")\\n    .option(\"kafka.bootstrap.servers\", KAFKA_BOOTSTRAP_SERVER)\\n    .option(\"subscribe\", KAFKA_TOPIC_NAME)\\n    .option(\"startingOffsets\", \"latest\")\\n    .load(\"streaming\")\\n    .selectExpr(\"CAST(value as STRING)\")\\n)\\n\\nschema = StructType([\\nStructField(\"name\",StringType(),True), \\nStructField(\"station_code\",StringType(),True), \\nStructField(\"ebike\",StringType(),True),\\nStructField(\"mechanical\", StringType(), True),\\nStructField(\"coordonnees_geo\", StringType(), True),\\nStructField(\"duedate\", StringType(), True),\\nStructField(\"numbikesavailable\", StringType(), True),\\nStructField(\"numdocksavailable\", StringType(), True),\\nStructField(\"capacity\", StringType(), True),\\nStructField(\"is_renting\", StringType(), True),\\nStructField(\"is_installed\", StringType(), True),\\nStructField(\"nom_arrondissement_communes\", StringType(), True),\\nStructField(\"is_returning\", StringType(), True),\\nStructField(\"geometry\", StringType(), True),\\nStructField(\"coordinates\", StringType(), True),\\nStructField(\"record_timestamp\", StringType(), True)])\\n\\n#jschema = schema_of_json(\\'{a: 1}\\', {\\'allowUnquotedFieldNames\\':\\'true\\'})\\n\\nprint(df.schema)\\nprint(df.isStreaming)\\n\\n#df2 = df.select(from_json(col(\"value\").cast(\"string\"), schema= schema).alias(\"new_value\"))\\nfinal_df = df.select(\\'value\\', F.get_json_object(\\'value\\', \\'value.fields\\').alias(\\'schemeCode\\'))\\n\\n#final_df.show(truncate=False)\\n\\nquery = final_df .writeStream .format(\"json\").option(\"path\", \"velib_data_clean_1\").option(\"checkpointLocation\", \"checkpoints\").start()\\n\\nquery.awaitTermination()\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "KAFKA_TOPIC_NAME = \"vlib_status_ville\"\n",
    "KAFKA_BOOTSTRAP_SERVER = \"51.38.185.58:9092\"\n",
    "    \n",
    "df = (\n",
    "    spark.readStream.format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", KAFKA_BOOTSTRAP_SERVER)\n",
    "    .option(\"subscribe\", KAFKA_TOPIC_NAME)\n",
    "    .option(\"startingOffsets\", \"latest\")\n",
    "    .load(\"streaming\")\n",
    "    .selectExpr(\"CAST(value as STRING)\")\n",
    ")\n",
    "\n",
    "schema = StructType([\n",
    "StructField(\"name\",StringType(),True), \n",
    "StructField(\"station_code\",StringType(),True), \n",
    "StructField(\"ebike\",StringType(),True),\n",
    "StructField(\"mechanical\", StringType(), True),\n",
    "StructField(\"coordonnees_geo\", StringType(), True),\n",
    "StructField(\"duedate\", StringType(), True),\n",
    "StructField(\"numbikesavailable\", StringType(), True),\n",
    "StructField(\"numdocksavailable\", StringType(), True),\n",
    "StructField(\"capacity\", StringType(), True),\n",
    "StructField(\"is_renting\", StringType(), True),\n",
    "StructField(\"is_installed\", StringType(), True),\n",
    "StructField(\"nom_arrondissement_communes\", StringType(), True),\n",
    "StructField(\"is_returning\", StringType(), True),\n",
    "StructField(\"geometry\", StringType(), True),\n",
    "StructField(\"coordinates\", StringType(), True),\n",
    "StructField(\"record_timestamp\", StringType(), True)])\n",
    "\n",
    "#jschema = schema_of_json('{a: 1}', {'allowUnquotedFieldNames':'true'})\n",
    "\n",
    "print(df.schema)\n",
    "print(df.isStreaming)\n",
    "\n",
    "#df2 = df.select(from_json(col(\"value\").cast(\"string\"), schema= schema).alias(\"new_value\"))\n",
    "final_df = df.select('value', F.get_json_object('value', 'value.fields').alias('schemeCode'))\n",
    "\n",
    "#final_df.show(truncate=False)\n",
    "\n",
    "query = final_df \\\n",
    ".writeStream \\\n",
    ".format(\"json\")\\\n",
    ".option(\"path\", \"velib_data_clean_1\")\\\n",
    ".option(\"checkpointLocation\", \"checkpoints\")\\\n",
    ".start()\n",
    "\n",
    "query.awaitTermination()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "EILCDu63cp6W"
   },
   "outputs": [],
   "source": [
    "raw_json = spark.read.json(\"test.json\", multiLine=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 452
    },
    "id": "Hs5YXLcVcl8E",
    "outputId": "38dd02ae-bb37-4086-fab4-2f2c5e64a5d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType([StructField('name', StringType(), True), StructField('datasetid', StringType(), True), StructField('datasetid', StringType(), True), StructField('fields', StructType([StructField('capacity', LongType(), True), StructField('coordonnees_geo', ArrayType(DoubleType(), True), True), StructField('duedate', StringType(), True), StructField('ebike', LongType(), True), StructField('is_installed', StringType(), True), StructField('is_renting', StringType(), True), StructField('is_returning', StringType(), True), StructField('mechanical', LongType(), True), StructField('name', StringType(), True), StructField('nom_arrondissement_communes', StringType(), True), StructField('numbikesavailable', LongType(), True), StructField('numdocksavailable', LongType(), True), StructField('stationcode', StringType(), True)]), True), StructField('geometry', StructType([StructField('coordinates', ArrayType(DoubleType(), True), True), StructField('type', StringType(), True)]), True), StructField('record_timestamp', StringType(), True), StructField('recordid', StringType(), True)])\n",
      "True\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-1ddb84b60972>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misrent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'OUI'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mech'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;34m.\u001b[0m\u001b[0mwriteStream\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"checkpointLocation\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"checkpoint/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"path\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"output_path/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputMode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"complete\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/streaming.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self, path, format, outputMode, partitionBy, queryName, **options)\u001b[0m\n\u001b[1;32m   1387\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueryName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqueryName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1388\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1389\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1322\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    194\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Data source csv does not support Complete output mode"
     ]
    }
   ],
   "source": [
    "KAFKA_TOPIC_NAME = \"vlib_status_ville\"\n",
    "KAFKA_BOOTSTRAP_SERVER = \"51.38.185.58:9092\"\n",
    "\n",
    "df = (\n",
    "spark.readStream.format(\"kafka\")\n",
    ".option(\"kafka.bootstrap.servers\", KAFKA_BOOTSTRAP_SERVER)\n",
    ".option(\"subscribe\", KAFKA_TOPIC_NAME)\n",
    ".option(\"startingOffsets\", \"latest\")\n",
    ".load()\n",
    ".withColumn(\"value\", from_json(col(\"value\").cast(\"string\"), raw_json.schema))\\\n",
    ".select(col('value.fields.name'), col('value.datasetid'), col('value.*'))\\\n",
    ")\n",
    "\n",
    "print(df.schema)\n",
    "print(df.isStreaming)\n",
    "\n",
    "rep = df.select(col('name').alias('name'),\n",
    "                col('fields.mechanical').alias('mech'),\n",
    "                col('fields.is_renting').alias('isrent'))\n",
    "    \n",
    "\n",
    "query = rep.filter(rep.isrent == 'OUI').groupBy('name').avg('mech') \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ITld9NgQfpre",
    "outputId": "1423239a-d4d4-4376-c190-481b053a8365"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%sql` not found.\n"
     ]
    }
   ],
   "source": [
    "%sql select * from memory_spark"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Mini_Projet.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
