{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7006e656",
   "metadata": {},
   "source": [
    "# Exercice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95acab1a",
   "metadata": {},
   "source": [
    "## Appliquer un traitement de comptage des occurrences de mot en temps réel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c8e8a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import\n",
    "import time\n",
    "import pyspark\n",
    "import findspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "565a5c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialisation de findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c354ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Création du SparkSession\n",
    "spark = SparkSession.builder.appName(\"Exemple Spark\").getOrCreate()\n",
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd4749f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc = StreamingContext(sc, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f4e91d",
   "metadata": {},
   "source": [
    "### Méthode 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "291f0c98",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TransformedDStream' object has no attribute 'saveAsTextFile'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m lines\u001b[38;5;241m.\u001b[39mpprint()\n\u001b[0;32m      4\u001b[0m counts \u001b[38;5;241m=\u001b[39m lines\u001b[38;5;241m.\u001b[39mflatMap(\u001b[38;5;28;01mlambda\u001b[39;00m line: line\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m))\\\n\u001b[0;32m      5\u001b[0m                   \u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m word: (word, \u001b[38;5;241m1\u001b[39m))\\\n\u001b[0;32m      6\u001b[0m                   \u001b[38;5;241m.\u001b[39mreduceByKey(\u001b[38;5;28;01mlambda\u001b[39;00m x, y: x \u001b[38;5;241m+\u001b[39m y)\n\u001b[1;32m----> 8\u001b[0m \u001b[43mcounts\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msaveAsTextFile\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfluxout/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(datetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;241m.\u001b[39mtime()))\n\u001b[0;32m      9\u001b[0m counts\u001b[38;5;241m.\u001b[39mpprint()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'TransformedDStream' object has no attribute 'saveAsTextFile'"
     ]
    }
   ],
   "source": [
    "lines = ssc.textFileStream(directory = \"C:/Users/yohan/OneDrive/Bureau/Formations/Cours/Spark/Exercices_SparkStreaming/test/\")\n",
    "lines.pprint()\n",
    "\n",
    "counts = lines.flatMap(lambda line: line.split(\" \"))\\\n",
    "                  .map(lambda word: (word, 1))\\\n",
    "                  .reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "counts.saveAsTextFile(\"fluxout/\" + str(datetime.now().time()))\n",
    "counts.pprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4aa7386",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = lines.flatMap(lambda line: line.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab35ba8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = words.map(lambda word: (word, 1))\n",
    "wordCounts = pairs.reduceByKey(lambda x, y: x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c8442f",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordCounts.saveAsTextFile(\"fluxout/\" + str(datetime.now().time()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b3d0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.start()\n",
    "ssc.awaitTermination() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570b8c42",
   "metadata": {},
   "source": [
    "### Méthode 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b3c32d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = ssc.textFileStream(\"C:/Users/yohan/OneDrive/Bureau/Formations/Cours/Spark/Exercices_SparkStreaming/test/\")\n",
    "lines.pprint()\n",
    "\n",
    "words = lines.flatMap(lambda line: line.split(\",\"))\n",
    "pairs = words.map(lambda word: (word, 1))\n",
    "wordCounts = pairs.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "def process(time, rdd):\n",
    "    df = sqlContext.createDataFrame(rdd)\n",
    "    df.registerTempTable(\"myCounts\")\n",
    "\n",
    "wordCounts.foreachRDD(process)\n",
    "ssc.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b16b2e",
   "metadata": {},
   "source": [
    "## Avec Spark Structured Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b607d128",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataStreamWriter' object has no attribute 'show'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 22>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m wordCounts \u001b[38;5;241m=\u001b[39m words\u001b[38;5;241m.\u001b[39mgroupBy(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mword\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mcount()\n\u001b[0;32m     21\u001b[0m  \u001b[38;5;66;03m# Start running the query that prints the running counts to the console\u001b[39;00m\n\u001b[0;32m     22\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[43mwordCounts\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m          \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwriteStream\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m          \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutputMode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcomplete\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m          \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconsole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m---> 26\u001b[0m \u001b[43m          \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m()\\\n\u001b[0;32m     27\u001b[0m           \u001b[38;5;241m.\u001b[39mstart()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataStreamWriter' object has no attribute 'show'"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql.functions import split\n",
    "\n",
    "\n",
    "# Create DataFrame representing the stream of input lines from connection to localhost:9999\n",
    "lines = spark \\\n",
    "         .readStream \\\n",
    "         .format(\"socket\") \\\n",
    "         .option(\"host\", \"localhost\") \\\n",
    "         .option(\"port\", 9999) \\\n",
    "         .load(\"C:/Users/yohan/OneDrive/Bureau/Formations/Cours/Spark/Exercices_SparkStreaming/test/fichier.txt\")\n",
    "\n",
    "# Split the lines into words\n",
    "words = lines.select(explode(split(lines.value, \" \")).alias(\"word\"))\n",
    "\n",
    "# Generate running word count\n",
    "wordCounts = words.groupBy(\"word\").count()\n",
    "\n",
    "\n",
    " # Start running the query that prints the running counts to the console\n",
    "query = wordCounts \\\n",
    "          .writeStream \\\n",
    "          .outputMode(\"complete\") \\\n",
    "          .format(\"console\") \\\n",
    "          .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78502418",
   "metadata": {},
   "outputs": [],
   "source": [
    "#time.sleep(6)\n",
    "#ssc.stop(stopSparkContext=True, stopGraceFully=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
